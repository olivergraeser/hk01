{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing Basic Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import jieba.analyse\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "## 1.1 Loading Training Data (For Input Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_csv('offsite-test-material/offsite-tagging-training-set.csv', encoding='utf8')\n",
    "training_df.index = training_df['id']\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, 'html5lib') #remove HTML tokens\n",
    "    text_only = soup.get_text() \n",
    "    text_normal_whitespace = re.sub(\"\\s\\s+\", \" \", text_only)\n",
    "    return text_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>clean_length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3443</th>\n",
       "      <td>3443</td>\n",
       "      <td>足球</td>\n",
       "      <td>利物浦重賽擊敗乙組仔　英足盃過關 英格蘭足總盃第三圈今晨重賽，貴為英超勁旅的利物浦上場被乙組...</td>\n",
       "      <td>利物浦重賽擊敗乙組仔　英足盃過關 英格蘭足總盃第三圈今晨重賽，貴為英超勁旅的利物浦上場被乙組...</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76056</th>\n",
       "      <td>76056</td>\n",
       "      <td>足球</td>\n",
       "      <td>【中超】恒大「暴力戰」絕殺國安　楊智反重力插水惹爭議（有片） 中超首輪賽事重頭戲，廣州恒大主...</td>\n",
       "      <td>【中超】恒大「暴力戰」絕殺國安　楊智反重力插水惹爭議（有片） 中超首輪賽事重頭戲，廣州恒大主...</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93405</th>\n",
       "      <td>93405</td>\n",
       "      <td>足球</td>\n",
       "      <td>【歐霸決賽】阿積士控球率起腳佔優　隊長卡拉臣輸波不服氣 阿積士以歐洲主要決賽最年輕、平均22...</td>\n",
       "      <td>【歐霸決賽】阿積士控球率起腳佔優　隊長卡拉臣輸波不服氣 阿積士以歐洲主要決賽最年輕、平均22...</td>\n",
       "      <td>959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26767</th>\n",
       "      <td>26767</td>\n",
       "      <td>足球</td>\n",
       "      <td>【歐國盃】韋莫斯澄清更衣室未內訌　盼以團結力量挫愛爾蘭 今晚3場直播\\r\\r\\nE組｜比利時...</td>\n",
       "      <td>【歐國盃】韋莫斯澄清更衣室未內訌　盼以團結力量挫愛爾蘭 今晚3場直播\\n\\nE組｜比利時Vs...</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20843</th>\n",
       "      <td>20843</td>\n",
       "      <td>梁振英</td>\n",
       "      <td>王維基參選　點解？ 王維基在宣布有意出選的記者會上，打出ABC，Anyone But CY的...</td>\n",
       "      <td>王維基參選　點解？ 王維基在宣布有意出選的記者會上，打出ABC，Anyone But CY的...</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id tags                                               text  \\\n",
       "id                                                                     \n",
       "3443    3443   足球  利物浦重賽擊敗乙組仔　英足盃過關 英格蘭足總盃第三圈今晨重賽，貴為英超勁旅的利物浦上場被乙組...   \n",
       "76056  76056   足球  【中超】恒大「暴力戰」絕殺國安　楊智反重力插水惹爭議（有片） 中超首輪賽事重頭戲，廣州恒大主...   \n",
       "93405  93405   足球  【歐霸決賽】阿積士控球率起腳佔優　隊長卡拉臣輸波不服氣 阿積士以歐洲主要決賽最年輕、平均22...   \n",
       "26767  26767   足球  【歐國盃】韋莫斯澄清更衣室未內訌　盼以團結力量挫愛爾蘭 今晚3場直播\\r\\r\\nE組｜比利時...   \n",
       "20843  20843  梁振英  王維基參選　點解？ 王維基在宣布有意出選的記者會上，打出ABC，Anyone But CY的...   \n",
       "\n",
       "                                              text_clean  clean_length  \n",
       "id                                                                      \n",
       "3443   利物浦重賽擊敗乙組仔　英足盃過關 英格蘭足總盃第三圈今晨重賽，貴為英超勁旅的利物浦上場被乙組...           369  \n",
       "76056  【中超】恒大「暴力戰」絕殺國安　楊智反重力插水惹爭議（有片） 中超首輪賽事重頭戲，廣州恒大主...           637  \n",
       "93405  【歐霸決賽】阿積士控球率起腳佔優　隊長卡拉臣輸波不服氣 阿積士以歐洲主要決賽最年輕、平均22...           959  \n",
       "26767  【歐國盃】韋莫斯澄清更衣室未內訌　盼以團結力量挫愛爾蘭 今晚3場直播\\n\\nE組｜比利時Vs...           782  \n",
       "20843  王維基參選　點解？ 王維基在宣布有意出選的記者會上，打出ABC，Anyone But CY的...          1281  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df['text_clean'] = training_df.apply(lambda _: remove_html(_['text']), axis=1)\n",
    "training_df['clean_length'] = training_df.apply(lambda _: len(_['text_clean']), axis=1)\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_cnt</th>\n",
       "      <th>record_cnt</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tags</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>梁振英</th>\n",
       "      <td>887021</td>\n",
       "      <td>929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>美國大選</th>\n",
       "      <td>993030</td>\n",
       "      <td>842</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>足球</th>\n",
       "      <td>1701491</td>\n",
       "      <td>2123</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      char_cnt  record_cnt  label_id\n",
       "tags                                \n",
       "梁振英     887021         929         0\n",
       "美國大選    993030         842         1\n",
       "足球     1701491        2123         2"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_group(row):\n",
    "    return pd.Series(dict(char_cnt=row['clean_length'].sum(), record_cnt=row.clean_length.count()))\n",
    "labels_df = pd.DataFrame(training_df.groupby(['tags']).apply(process_group))\n",
    "labels_df['label_id'] = pd.Categorical(labels_df.index).codes\n",
    "label_dict = {a: b.label_id for a, b in labels_df.iterrows()}\n",
    "label_id_dict = {b.label_id: a for a, b in labels_df.iterrows()}\n",
    "labels = list(label_dict.keys())\n",
    "labels_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Loading Data (For Frequency Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = defaultdict(list)\n",
    "with open('offsite-test-material/offsite-tagging-training-set.csv', 'r', encoding='utf8') as f:\n",
    "    file_reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    next(file_reader)\n",
    "    for row in file_reader:\n",
    "        text_dict[row[1]].append(remove_html(row[2]))\n",
    "\n",
    "fulltext_dict = {k: '\\n'.join([_ for _ in v]) for k, v in text_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following categories:\n",
      "足球: 2123 fragments with 1703613 characters\n",
      "梁振英: 929 fragments with 887949 characters\n",
      "美國大選: 842 fragments with 993871 characters\n"
     ]
    }
   ],
   "source": [
    "print('Found the following categories:\\n{}'.format('\\n'.join(['{}: {} fragments with {} characters'\n",
    "                                                               .format(k, len(text_dict[k]), len(fulltext_dict[k]))\n",
    "                                                               for k in text_dict.keys() ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we have twice the frequency of articles related to soccer than to either the outgoing CE or US elections. This is a bit tricky in terms of maximum TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Selecting most relevant tokens\n",
    "I am building a TD-IDF-esque model, for which I will select the most 'relevant' tokens as features. Relevance here is defined as the highest ratio of frequency in the relevant 'term' over the frequency in the overall 'document'. A 'term' here is the union of all segments that belong to a single categroy. The document is the union of all segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary for occurrence of short tokens in each classified doc\n",
    "labeldicts_short = {_: (defaultdict(float), 0) for _ in labels} \n",
    "#dictionary for occurrence of long tokens in each classified doc\n",
    "labeldicts_long = {_: (defaultdict(float), 0) for _ in labels} \n",
    "# dictionary for occurrence of short tokens in the whole document\n",
    "docdict_short = defaultdict(float), 0 \n",
    "# dictionary for occurrence of long tokens in the whole document\n",
    "docdict_long = defaultdict(float), 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.998 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# Counting token frequency\n",
    "training_clean = list()\n",
    "for label, combined_text in fulltext_dict.items():\n",
    "    short_tokens = jieba.cut(combined_text, cut_all=True)\n",
    "    for token in short_tokens:\n",
    "        labeldicts_short[label][0][token] += 1\n",
    "        docdict_short[0][token] += 1\n",
    "        \n",
    "    long_tokens = jieba.cut(combined_text, cut_all=False)\n",
    "    for token in long_tokens:\n",
    "        labeldicts_long[label][0][token] += 1\n",
    "        docdict_long[0][token] += 1\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating term/document length\n",
    "for label in labels:\n",
    "    labeldicts_long[label] = labeldicts_long[label][0], sum(labeldicts_long[label][0].values())\n",
    "    labeldicts_short[label] = labeldicts_short[label][0], sum(labeldicts_short[label][0].values())\n",
    "    \n",
    "docdict_long = docdict_long[0], sum(docdict_long[0].values())\n",
    "docdict_short = docdict_short[0], sum(docdict_short[0].values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0976038838647515, 29.0, 29.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper function that returns the highest TFIDF of a token. \n",
    "# highly relevant tokens will have maximum TDIDFs of 2-3,  \n",
    "# they exclusively occur in fragments of one class, but the size of the term length differs\n",
    "# irrelevant tokens will have uniform TFIDFs of 1 (they occur everywhere with the same frequencyy)\n",
    "def relative_frequency(token, classdicts, docdict, docdict_total=None):\n",
    "    occurrences = [(classdict[0][token], classdict[1]) for classdict in classdicts if token in classdict[0]]\n",
    "    if occurrences:\n",
    "        max_occurence, term_length = max(occurrences, key=lambda _: _[0]/_[1])\n",
    "        total_occurrence, doc_length  = docdict[0][token], docdict[1]\n",
    "        tf = (max_occurence/term_length)\n",
    "        df = (total_occurrence/doc_length)\n",
    "        return (tf/df, max_occurence, total_occurrence)\n",
    "    else:\n",
    "        print(token)\n",
    "        return 0, 0, docdict[0].get(token, 0)\n",
    "\n",
    "relative_frequency('重賽', labeldicts_long.values(), docdict_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevance_cutoff = 1.8\n",
    "occurrent_cutoff = 50\n",
    "\n",
    "short_classdicts = labeldicts_short.values()\n",
    "long_classdicts = labeldicts_long.values()\n",
    "maxfreq_short = {key: relative_frequency(key, short_classdicts, docdict_short) for key in docdict_short[0].keys()}\n",
    "maxfreq_long = {key: relative_frequency(key, long_classdicts, docdict_long) for key in docdict_long[0].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_tokens_short_list = sorted([key for key, value in maxfreq_short.items() \n",
    "                                     if (value[0] > relevance_cutoff and\n",
    "                                         value[2] > occurrent_cutoff and\n",
    "                                         key.isalpha())])\n",
    "\n",
    "relevant_tokens_long_list = sorted([key for key, value in maxfreq_long.items() \n",
    "                                    if (value[0] > relevance_cutoff and\n",
    "                                        value[2] > occurrent_cutoff and\n",
    "                                        key.isalpha())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Creating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentence, tokenlist, cut_all=False):\n",
    "    a = defaultdict(int)\n",
    "    tokens = jieba.cut(sentence, cut_all=cut_all)\n",
    "    for token in tokens:\n",
    "        a[token] += 1\n",
    "    out_dict = {_: a.get(_, 0) for _ in tokenlist}\n",
    "    return pd.Series(out_dict)\n",
    "\n",
    "occ_input_long = pd.DataFrame(training_df.text_clean.apply(\n",
    "    lambda _: sentence_to_vector(_, relevant_tokens_long_list)))\n",
    "\n",
    "occ_input_short = pd.DataFrame(training_df.text_clean.apply(\n",
    "    lambda _: sentence_to_vector(_, relevant_tokens_short_list, cut_all=True)))\n",
    "\n",
    "data_target = pd.DataFrame(training_df.merge(labels_df, how='inner', left_on='tags', right_index=True)['label_id'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_share = 0.2\n",
    "indices = list(occ_input_long.index)\n",
    "test_indices = random.sample(indices, int(len(indices)*test_share))\n",
    "train_indices = [_ for _ in indices if _ not in test_indices]\n",
    "training_data_long = occ_input_long.loc[train_indices]\n",
    "training_data_short = occ_input_short.loc[train_indices]\n",
    "holdout_data_long = occ_input_long.loc[test_indices]\n",
    "holdout_data_short = occ_input_short.loc[test_indices]\n",
    "training_target = data_target.loc[train_indices]\n",
    "holdout_target = data_target.loc[test_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Training Models\n",
    "## 2.1 Standard RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC = RandomForestClassifier()\n",
    "RFC.fit(np.asarray(training_data_short), np.asarray(training_target).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98971722365038561"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(RFC.predict(np.asarray(holdout_data_short)), holdout_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Gradient Boosted Classifier (Standard SkLearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(np.asarray(training_data_short), np.asarray(training_target).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98971722365038561"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(gbc.predict(np.asarray(holdout_data_short)), holdout_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.DataFrame([[0,0,0,1,1],[0,0,0,1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto',\n",
       "  kernel=<function cosine_similarity at 0x12f1779d8>, max_iter=-1,\n",
       "  probability=False, random_state=None, shrinking=True, tol=0.001,\n",
       "  verbose=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel=cosine_similarity)\n",
    "svc.fit(training_data_long, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.loc[25131].label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following text was classified as 梁振英, whereas it should be 美國大選. \n",
      "【早晨精讀】施嘉莉襲港　港姐面試鬥「靚」　端午有活雞　 施嘉莉現身油麻地拍《攻殼》荷李活女星施嘉莉祖安遜（Scarlett Johansson） 昨日忽然現身香港拍攝新片《攻殼機動隊》（Ghost in the Shell），《香港01》更拍得獨家照片。其後施嘉莉在接近30高溫度穿厚重大褸拍攝 施嘉莉現身油麻地拍《攻殼》 領展檢討外判制 端午恢復活雞供應\n",
      "The following text was classified as 足球, whereas it should be 梁振英. \n",
      "趙世光靈堂白玫瑰布置　何琍琍說丈夫遺願：香港成為世界航運中心 船王趙從衍之子趙世光於香港殯儀館設靈，現場有過百花牌，禮堂以白玫瑰布置，外面有十多名穿黑色西裝的工作人員，有時見到記者拍攝會上前出手阻止。\n",
      "\n",
      "\n",
      "\n",
      "下午約三時半，趙世光遺孀何琍琍一臉憔悴現身殯儀館，何琍琍表示現在心情已經好好多，多謝大家關心， 船王趙從衍之子趙世光於香港殯儀館設靈，現場有過百花牌，禮堂以白玫瑰布置，外面有十多名穿黑色西裝的工作人員，有時見到記者拍攝會上前出手阻止。\n",
      "\n",
      "\n",
      "\n",
      "下午約三時半，趙世光遺孀何琍琍一臉憔悴現身殯儀館，何琍琍表示現在心情已經好好多，多謝大家關心，又透露喪禮會以天主教儀式進行，問到趙先生可有遺願？她說：「希望香港成為世界航運中心。」果然不愧是船王之子。\n",
      "The following text was classified as 足球, whereas it should be 美國大選. \n",
      "《紐時》華裔編輯街頭受辱　潑婦大罵︰滾回中國！ 羅麥可的公開信是寫給那個出言不遜的婦人，他在文中寫道︰「我們當時剛剛離開教堂，我和我的家庭以及一些朋友正在曼克頓的上東區。那時我們去吃午餐，嘗試看看街尾一間韓國餐廳有沒有空位。你正在趕路，那時正在下雨，我們的嬰兒車以及一群嘈吵的亞洲人擋住你 羅麥可的公開信是寫給那個出言不遜的婦人，他在文中寫道︰「我們當時剛剛離開教堂，我和我的家庭以及一些朋友正在曼克頓的上東區。那時我們去吃午餐，嘗試看看街尾一間韓國餐廳有沒有空位。你正在趕路，那時正在下雨，我們的嬰兒車以及一群嘈吵的亞洲人擋住你的去路。但坦白說，當你從不遠處向我們大喝︰『滾回中國！』，我實感到震驚。」他的7歲女兒也聽到這句話，並不斷問他︰「為何她說『滾回中國！』（go back to China）而不是『走去中國！』（go to China）？我們又不是來自中國。」\n",
      "\n",
      "\n",
      "\n",
      "羅麥可當時沒有噤聲，立即向婦人回敬一句︰「我生於這個國家！」他自覺這句話挺愚蠢的，他怎能證明自己生於這裡呢？羅麥可續寫道道︰「當然，這並不是我第一次遇到這樣的種族侮辱。你問問任何一名亞裔美國人，他們隨時可回憶起在校園被嘲弄、在街頭又或者在雜貨店碰上的惱人遭遇。當我在Tiwtter貼文訴說這件事的經過，一大堆人以他們的經驗回應我。」 羅麥可的經歷在Twitter獲得廣大迴響，紐約市市長白思豪也作出回應。他指，羅麥可根本不須確認他在這裡出生，這裏每個人都屬於紐約市，不屬於這裏的是他聽到的言論。羅麥可也在facebook及其公開信的意見欄得到很多回應。其中一個回應，是一個亞裔男生分享他因種族而面對約會的困難。他在Twitter寫道︰「作為一個亞裔男性去找到象，你會看到很多人簡介資料中的偏好列出『所有美國人』，但你卻會收到像『不要中國人』的回應。」 （CNN） ▲很多亞裔美國人對羅麥可的事件都深同感受，部分在Twitter貼文撰寫其遭歧視經歷並加上「＃Thisis2016」標籤的人拍片聲演他們的貼文。\n",
      "The following text was classified as 足球, whereas it should be 美國大選. \n",
      "【搞手筆記】01烽火論壇直播背後 直播節目大家看得多，一個直播節目到底需要多少準備工夫？今次搞手筆記帶你窺探01烽火論壇@白宮新主人直播背後的一面。 大光燈背後，台前幕後都為美國大選特備直播節目01烽火論壇作最後準備。在直播開始前三小時已有初步點票結果，雖然大家都跌到一地眼鏡，但新一代白宮主人之爭將告落幕。對於我們一眾工作人員來說，輪到烽火論壇直播的Show Time了。要做一個直播節目 想重溫論壇精華片段<按此>\n",
      "\n",
      "\n",
      "\n",
      "  大光燈背後，台前幕後都為美國大選特備直播節目01烽火論壇作最後準備。在直播開始前三小時已有初步點票結果，雖然大家都跌到一地眼鏡，但新一代白宮主人之爭將告落幕。對於我們一眾工作人員來說，輪到烽火論壇直播的Show Time了。 要做一個直播節目其實牽涉很多技術要求，多個部門包括聲畫部、網絡及系統支援部、數碼平台部、編採部、網編部、市場推廣部等都有參與事前籌備工作，希望確保整個直播流程暢順。直播前一天，我們亦特別到01空間進行了實地彩排。 第一次與謝志峰先生合作，擁有豐富主持經驗的他非常淡定，能準確掌握直播的時間，包括分配各題目的討論時間和作總結，與他合作可以說是獲益良多。\n",
      "The following text was classified as 美國大選, whereas it should be 梁振英. \n",
      "【01觀點】由奧巴馬的遺憾　看國情咨文對香港的最大啟示 美國總統奧巴馬和香港特首梁振英本港時間周三幾乎在同一時間發表施政演講，確是一個鮮見的巧合。當然，梁振英只是一位管治700多萬人的地方首長，與奧巴馬這位需要管治3億人的超級大國領袖所面對的問題截然不同，很難將兩者的施政演說直接比較，何況兩者的 美國總統奧巴馬和香港特首梁振英本港時間周三幾乎在同一時間發表施政演講，確是一個鮮見的巧合。當然，梁振英只是一位管治700多萬人的地方首長，與奧巴馬這位需要管治3億人的超級大國領袖所面對的問題截然不同，很難將兩者的施政演說直接比較，何況兩者的水平級數，也實在無法相提並論。不過奧巴馬在今次國情咨文的其中一番話，確實值得梁振英以至香港相關各方認真思考。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "奧巴馬在演說中，談到了不少願景，他告訴美國人，未來可以是美好的，但前提是要各方能一起努力，「只有修補好我們的政治，這才能發生」。奧巴馬坦言，過去7年自己作為美國總統的其中一個遺憾，「是黨派之間的仇恨和猜疑，並沒有好轉，反而在惡化」。 儘管問題出現的背景迥異，但政治兩極化變本加厲的情況，確實同樣困擾著香港和美國。梁振英在當選後曾聲言，再沒有梁營又或唐營，只有「香港營」，然而香港社會的政治撕裂，在過去數年無疑是在嚴重惡化，佔領運動後的黃藍撕裂，更是完全沒見修補。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "至於美國的情況也同樣糟糕。2008年奧巴馬競選總統時，也曾聲言要團結國家，「我們並不是一堆紅州和藍州的湊合，我們就是美利堅合眾國」，然而7年過去，美國政治兩極化的情況，只有惡化而完全沒有改善可言。直到今時今日，很多保守派人士仍舊是那麼憎恨奧巴馬，認為他力推醫保是在搞「社會主義」；自2010年共和黨重奪眾議院控制權以來，立法與行政機關的對峙，也令到奧巴馬施政極受制肘。在今次演說中，奧巴馬多次贏得熱烈民主黨議員的熱烈掌聲，與在場共和黨議員的沉默形成強烈對比，已是政治兩極化的最佳寫照。 作為任內最後一份國情咨文，奧巴馬並沒有迴避自己在團結國家收窄黨派分歧上的失敗；更重要的是，他沒有將責任一面倒地歸咎任何一方，反而承認自己不足，「毫無疑問，一位像林肯或羅斯福般本事的總統，也許能更佳地連起這道鴻溝。我保證只要一天我在當總統，我仍會繼續努力做好些」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "當然，政客的言辭往往比幹起來的漂亮，奧巴馬也不例外；聽在共和黨及保守派耳中，奧巴馬這番話，也許只是在惺惺在態，但至少他有嘗試表現得較為謙遜、避免擺出一副戰鬥格，這已是值得肯定。 奧巴馬在演說中談到，美國人需要更好的政治，但要實現這目標，單單是換議員或總統是不足夠的，「必須改變我們的制度」，包括改革選舉方法，從而更好地反映民意；倘若社會大眾感到無能為力，「沮喪情緒滋長，就會出現一些聲音，驅使我們再度跌進不同群體壁壘當中，將那些外表看來不像我們、祈禱時不像我們、投票時不像我們，又或並不擁有相似背景的國民同胞，作為代罪羔羊。我們不能走這樣的一條路」，「當只有最極端的聲音受到注視，我們的公共生活就會敗壞；當一般人都覺得自己的聲音無關痛癢、制度明顯偏向富人權貴或某些特殊利益，民主就會崩壞。」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "奧巴馬這番話，不僅是想向公眾點出特朗普之流利用社會挫折感去煽動族群仇恨的禍害，同時亦點出了改革制度從而更好地反映民意的重要性。正如奧巴馬所言，「更好的政治並不代表要同意每一件事」，「但民主需要國民之間有基本的信任聯繫，若我們認為，那些不同意我們的人，都是出於惡意動機，又或我們的政敵全都不愛國，這樣是行不通的。沒有妥協的意願，只願聽同意自己的聲音，又或連最基本的事實也被質疑，民主會陷入停頓」。\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "誠然，奧巴馬自己也無法在美國做到凝聚共識，但他這番忠告，對美國社會固然適用，對香港又何嘗不是？\n",
      "The following text was classified as 足球, whereas it should be 梁振英. \n",
      "傑志中心讓路起樓　醫學界「足球員」陳沛然嘆：土地問題 傳媒日前揭發房屋署向沙田區議員摸底，擬收回港超聯球隊傑志的沙田石門訓練中心地皮建公屋，梁振英承認有關構思。熱愛足球的候任立法會醫學界議員陳沛然對事件感到可惜，引用「橫洲事件」作例，指香港不是沒有土地，認為政府「非不能也，實不為也」。 傳媒日前揭發房屋署向沙田區議員摸底，擬收回港超聯球隊傑志的沙田石門訓練中心地皮建公屋，引起公眾非議。曾稱體育對香港沒有經濟貢獻的特首梁振英昨（10月3日）首承認有相關想法，若落實方案，會先為傑志換地才收回地皮。\n",
      "\n",
      "\n",
      "\n",
      "曾在醫管局聯賽中奪冠， 傳媒日前揭發房屋署向沙田區議員摸底，擬收回港超聯球隊傑志的沙田石門訓練中心地皮建公屋，引起公眾非議。曾稱體育對香港沒有經濟貢獻的特首梁振英昨（10月3日）首承認有相關想法，若落實方案，會先為傑志換地才收回地皮。\n",
      "\n",
      "\n",
      "\n",
      "曾在醫管局聯賽中奪冠，候任立法會醫學界議員陳沛然今（10月4日）於網誌撰文，對近日傑志要讓路起樓感到可惜，又引用橫洲事件的棕地，指香港不是沒有土地，認為政府「非不能也，實不為也」。 陳沛然發表題為《我，支持保留賽馬會傑志中心，支持香港足球》的個人網誌，憶起自己今年4月曾於傑志中心踢比賽奪冠，當時陣中隊長由中場拉弓遠射中楣，感到該球場其實比較細，與香港一樣土地不足，但面積僅15,000平方米的傑志中心，已經包含了兩個細足球場、辦公室、更衣室、醫療室、健身室、小型觀眾席和商店，麻雀雖小，五臟俱全。 熱愛足球的陳指自己年紀大了，在石地場踢波後膝蓋會痛，也怕落地受傷影響工作，因此喜歡在11人足球草場上踢波。但他坦言要訂場踢波難，要訂11人足球草場超級難，過去曾參加一個比賽因訂場問題，從分組到淘汰賽，足足花了兩年時間才踢完。\n",
      "\n",
      "\n",
      "\n",
      "曾因租場難　兩年才踼完聯賽\n",
      "\n",
      "\n",
      "\n",
      "陳沛然指香港的土地問題，已影響了政治、房屋、醫療、教育、社福、骨灰龕位等多個範疇，如今更令足球受牽連，但認為香港並非沒有土地，是「非不能也，實不為也，最近橫洲事件的棕地就是一例」。陳的友人曾提議發展5人足球，陳表示認同，認為5人足球場可建在室內或天台，節省地方，而自己亦會親身行動支持「賽馬會傑志中心」，於本月底再到訪踢波，希望大家能多些關心香港足球。\n"
     ]
    }
   ],
   "source": [
    "prediction = svc.predict(np.asarray(holdout_data_long))\n",
    "acs = accuracy_score(prediction, (holdout_target))\n",
    "results = holdout_target.copy()\n",
    "results['prediction'] = prediction\n",
    "misclassified_ids = list(results[results['prediction']!=results['label_id']].index)\n",
    "for misclassified_id in misclassified_ids:\n",
    "    print('The following text was classified as {}, whereas it should be {}. \\n{}'\n",
    "          .format(label_id_dict[results.loc[misclassified_id].prediction],\n",
    "                  label_id_dict[results.loc[misclassified_id].label_id],\n",
    "                  training_df.loc[misclassified_id].text_clean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25131, 7577, 2079, 46523]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(results[results['prediction']!=results['label_id']].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC has been classified as  2 and should be  label_id\n"
     ]
    }
   ],
   "source": [
    "for input, prediction, label in zip(holdout_data_long, svc.predict(np.asarray(holdout_data_long)), holdout_target):\n",
    "    if prediction != label:\n",
    "        print(input, 'has been classified as ', prediction, 'and should be ', label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id\n",
       " 6000     True\n",
       " 26719    True\n",
       " 19573    True\n",
       " 53668    True\n",
       " 6406     True\n",
       " 12543    True\n",
       " 62329    True\n",
       " 66744    True\n",
       " 9917     True\n",
       " 20527    True\n",
       " 68923    True\n",
       " 77436    True\n",
       " 16598    True\n",
       " 19781    True\n",
       " 91115    True\n",
       " 88354    True\n",
       " 68969    True\n",
       " 44290    True\n",
       " 37767    True\n",
       " 2326     True\n",
       " 5080     True\n",
       " 79183    True\n",
       " 5420     True\n",
       " 53153    True\n",
       " 11613    True\n",
       " 51738    True\n",
       " 63512    True\n",
       " 43580    True\n",
       " 18526    True\n",
       " 59150    True\n",
       "          ... \n",
       " 67307    True\n",
       " 50617    True\n",
       " 60829    True\n",
       " 66309    True\n",
       " 86261    True\n",
       " 3008     True\n",
       " 79780    True\n",
       " 61992    True\n",
       " 33554    True\n",
       " 49605    True\n",
       " 24839    True\n",
       " 18328    True\n",
       " 11198    True\n",
       " 21817    True\n",
       " 68011    True\n",
       " 46754    True\n",
       " 56989    True\n",
       " 90441    True\n",
       " 3324     True\n",
       " 19436    True\n",
       " 9794     True\n",
       " 50244    True\n",
       " 73708    True\n",
       " 15497    True\n",
       " 51957    True\n",
       " 80969    True\n",
       " 30346    True\n",
       " 73633    True\n",
       " 6590     True\n",
       " 48924    True\n",
       " Length: 778, dtype: bool]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[holdout_target['label_id']==holdout_target['prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
