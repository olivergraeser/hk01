{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing Basic Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import jieba.analyse\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "## 1.1 Loading Training Data (For Input Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_csv('offsite-test-material/offsite-tagging-training-set.csv', encoding='utf8')\n",
    "training_df.index = training_df['id']\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, 'html5lib') #remove HTML tokens\n",
    "    text_only = soup.get_text() \n",
    "    text_normal_newline = re.sub(\"\\n\\n+\", \"\\n\", text_only)\n",
    "    text_normal_space = re.sub(\"\\s\\s+\", \" \", text_normal_newline)\n",
    "    return text_normal_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['text_clean'] = training_df.apply(lambda _: remove_html(_['text']), axis=1)\n",
    "training_df['clean_length'] = training_df.apply(lambda _: len(_['text_clean']), axis=1)\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_group(row):\n",
    "    return pd.Series(dict(char_cnt=row['clean_length'].sum(), record_cnt=row.clean_length.count()))\n",
    "labels_df = pd.DataFrame(training_df.groupby(['tags']).apply(process_group))\n",
    "labels_df['label_id'] = pd.Categorical(labels_df.index).codes\n",
    "label_dict = {a: b.label_id for a, b in labels_df.iterrows()}\n",
    "label_id_dict = {b.label_id: a for a, b in labels_df.iterrows()}\n",
    "labels = list(label_dict.keys())\n",
    "labels_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Loading Data (For Frequency Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_dict = defaultdict(list)\n",
    "with open('offsite-test-material/offsite-tagging-training-set.csv', 'r', encoding='utf8') as f:\n",
    "    file_reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    next(file_reader)\n",
    "    for row in file_reader:\n",
    "        text_dict[row[1]].append(remove_html(row[2]))\n",
    "\n",
    "fulltext_dict = {k: '\\n'.join([_ for _ in v]) for k, v in text_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Found the following categories:\\n{}'.format('\\n'.join(['{}: {} fragments with {} characters'\n",
    "                                                               .format(k, len(text_dict[k]), len(fulltext_dict[k]))\n",
    "                                                               for k in text_dict.keys() ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we have twice the frequency of articles related to soccer than to either the outgoing CE or US elections. This is a bit tricky in terms of maximum TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Selecting most relevant tokens\n",
    "I am building a TD-IDF-esque model, for which I will select the most 'relevant' tokens as features. Relevance here is defined as the highest ratio of frequency in the relevant 'term' over the frequency in the overall 'document'. A 'term' here is the union of all segments that belong to a single categroy. The document is the union of all segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dictionary for occurrence of short tokens in each classified doc\n",
    "labeldicts_short = {_: (defaultdict(float), 0) for _ in labels} \n",
    "#dictionary for occurrence of long tokens in each classified doc\n",
    "labeldicts_long = {_: (defaultdict(float), 0) for _ in labels} \n",
    "# dictionary for occurrence of short tokens in the whole document\n",
    "docdict_short = defaultdict(float), 0 \n",
    "# dictionary for occurrence of long tokens in the whole document\n",
    "docdict_long = defaultdict(float), 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting token frequency\n",
    "training_clean = list()\n",
    "for label, combined_text in fulltext_dict.items():\n",
    "    short_tokens = jieba.cut(combined_text, cut_all=True)\n",
    "    for token in short_tokens:\n",
    "        labeldicts_short[label][0][token] += 1\n",
    "        docdict_short[0][token] += 1\n",
    "        \n",
    "    long_tokens = jieba.cut(combined_text, cut_all=False)\n",
    "    for token in long_tokens:\n",
    "        labeldicts_long[label][0][token] += 1\n",
    "        docdict_long[0][token] += 1\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating term/document length\n",
    "for label in labels:\n",
    "    labeldicts_long[label] = labeldicts_long[label][0], sum(labeldicts_long[label][0].values())\n",
    "    labeldicts_short[label] = labeldicts_short[label][0], sum(labeldicts_short[label][0].values())\n",
    "    \n",
    "docdict_long = docdict_long[0], sum(docdict_long[0].values())\n",
    "docdict_short = docdict_short[0], sum(docdict_short[0].values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that returns the highest TFIDF of a token. \n",
    "# highly relevant tokens will have maximum TDIDFs of 2-3,  \n",
    "# they exclusively occur in fragments of one class, but the size of the term length differs\n",
    "# irrelevant tokens will have uniform TFIDFs of 1 (they occur everywhere with the same frequencyy)\n",
    "def relative_frequency(token, classdicts, docdict, docdict_total=None):\n",
    "    occurrences = [(classdict[0][token], classdict[1]) for classdict in classdicts if token in classdict[0]]\n",
    "    if occurrences:\n",
    "        max_occurence, term_length = max(occurrences, key=lambda _: _[0]/_[1])\n",
    "        total_occurrence, doc_length  = docdict[0][token], docdict[1]\n",
    "        tf = (max_occurence/term_length)\n",
    "        df = (total_occurrence/doc_length)\n",
    "        return (tf/df, max_occurence, total_occurrence)\n",
    "    else:\n",
    "        print(token)\n",
    "        return 0, 0, docdict[0].get(token, 0)\n",
    "\n",
    "relative_frequency('重賽', labeldicts_long.values(), docdict_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevance_cutoff = 1.8\n",
    "occurrent_cutoff = 50\n",
    "\n",
    "short_classdicts = labeldicts_short.values()\n",
    "long_classdicts = labeldicts_long.values()\n",
    "maxfreq_short = {key: relative_frequency(key, short_classdicts, docdict_short) for key in docdict_short[0].keys()}\n",
    "maxfreq_long = {key: relative_frequency(key, long_classdicts, docdict_long) for key in docdict_long[0].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_tokens_short_list = sorted([key for key, value in maxfreq_short.items() \n",
    "                                     if (value[0] > relevance_cutoff and\n",
    "                                         value[2] > occurrent_cutoff and\n",
    "                                         key.isalpha())])\n",
    "\n",
    "relevant_tokens_long_list = sorted([key for key, value in maxfreq_long.items() \n",
    "                                    if (value[0] > relevance_cutoff and\n",
    "                                        value[2] > occurrent_cutoff and\n",
    "                                        key.isalpha())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Creating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentence, tokenlist, cut_all=False):\n",
    "    a = defaultdict(int)\n",
    "    tokens = jieba.cut(sentence, cut_all=cut_all)\n",
    "    for token in tokens:\n",
    "        a[token] += 1\n",
    "    out_dict = {_: a.get(_, 0) for _ in tokenlist}\n",
    "    return pd.Series(out_dict)\n",
    "\n",
    "occ_input_long = pd.DataFrame(training_df.text_clean.apply(\n",
    "    lambda _: sentence_to_vector(_, relevant_tokens_long_list)))\n",
    "\n",
    "occ_input_short = pd.DataFrame(training_df.text_clean.apply(\n",
    "    lambda _: sentence_to_vector(_, relevant_tokens_short_list, cut_all=True)))\n",
    "\n",
    "data_target = pd.DataFrame(training_df.merge(labels_df, how='inner', left_on='tags', right_index=True)['label_id'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_share = 0.2\n",
    "indices = list(occ_input_long.index)\n",
    "test_indices = random.sample(indices, int(len(indices)*test_share))\n",
    "train_indices = [_ for _ in indices if _ not in test_indices]\n",
    "training_data_long = occ_input_long.loc[train_indices]\n",
    "training_data_short = occ_input_short.loc[train_indices]\n",
    "holdout_data_long = occ_input_long.loc[test_indices]\n",
    "holdout_data_short = occ_input_short.loc[test_indices]\n",
    "training_target = data_target.loc[train_indices]\n",
    "holdout_target = data_target.loc[test_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Training Models\n",
    "## 2.0 Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_misclassification(id_, prediction, holdout):\n",
    "    relevant_text = training_df.loc[id_]['text_clean']\n",
    "    provided_label = training_df.loc[id_]['tags']\n",
    "    predicted_label = label_id_dict[prediction[holdout.index.get_loc(id_)]]\n",
    "    print('The following text was classified as {0}, but labelled as {1}:\\n{2}'\n",
    "          .format(predicted_label, provided_label, relevant_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Standard RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_short = RandomForestClassifier()\n",
    "RFC_short.fit(np.asarray(training_data_short), np.asarray(training_target).ravel())\n",
    "rfc_prediction_short = RFC_short.predict(np.asarray(holdout_data_short))\n",
    "misclassified_ids_short = list(sorted(holdout_target[rfc_prediction_short!=holdout_target['label_id']].index))\n",
    "print('A RandomForest Classifier reached an accuracy score of {0:.4f} for short tokens.\\nThis means that a total of {1}'\n",
    "      ' fragments (out of {2} fragments in the holdout sample) was misclassified.\\nThe misclassified ids are:\\n{3}'\n",
    "      .format(accuracy_score(rfc_prediction_short, holdout_target),\n",
    "              len(misclassified_ids_short),\n",
    "              len(holdout_target),\n",
    "             ', '.join(str(_) for _ in misclassified_ids_short)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_long = RandomForestClassifier()\n",
    "RFC_long.fit(np.asarray(training_data_long), np.asarray(training_target).ravel())\n",
    "rfc_prediction_long = RFC_long.predict(np.asarray(holdout_data_long))\n",
    "misclassified_ids_long = list(holdout_target[rfc_prediction_long!=holdout_target['label_id']].index)\n",
    "print('A RandomForest Classifier reached an accuracy score of {0:.4f} for short tokens.\\nThis means that a total of {1}'\n",
    "      ' fragments (out of {2} fragments in the holdout sample) was misclassified.\\nThe misclassified ids are:\\n{3}'\n",
    "      .format(accuracy_score(rfc_prediction_long, holdout_target),\n",
    "              len(misclassified_ids_long),\n",
    "              len(holdout_target),\n",
    "             ', '.join(str(_) for _ in misclassified_ids_long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_id = 2079\n",
    "explain_misclassification(misclassified_id, rfc_prediction_short, holdout_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Gradient Boosted Classifier (Standard SkLearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(np.asarray(training_data_short), np.asarray(training_target).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(gbc.predict(np.asarray(holdout_data_short)), holdout_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=pd.DataFrame([[0,0,0,1,1],[0,0,0,1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel=cosine_similarity)\n",
    "svc.fit(training_data_long, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[25131].label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = svc.predict(np.asarray(holdout_data_long))\n",
    "acs = accuracy_score(prediction, (holdout_target))\n",
    "results = holdout_target.copy()\n",
    "results['prediction'] = prediction\n",
    "misclassified_ids = list(results[results['prediction']!=results['label_id']].index)\n",
    "for misclassified_id in misclassified_ids:\n",
    "    print('The following text was classified as {}, whereas it should be {}. \\n{}'\n",
    "          .format(label_id_dict[results.loc[misclassified_id].prediction],\n",
    "                  label_id_dict[results.loc[misclassified_id].label_id],\n",
    "                  training_df.loc[misclassified_id].text_clean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(results[results['prediction']!=results['label_id']].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for input, prediction, label in zip(holdout_data_long, svc.predict(np.asarray(holdout_data_long)), holdout_target):\n",
    "    if prediction != label:\n",
    "        print(input, 'has been classified as ', prediction, 'and should be ', label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[holdout_target['label_id']==holdout_target['prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
