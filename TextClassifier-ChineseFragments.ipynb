{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing Basic Libraries, Definitions & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import jieba.analyse\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_seed = 64 #Can be anyvalue - but it is important to set one to keep training/holdout set constant\n",
    "relevance_cutoff = 1.8 # required TFIDF value to be included in the relevant token set\n",
    "occurrent_cutoff = 50 #required document occurrence to be included in the relevant token set\n",
    "test_share = 0.2 #share of the holdout sample vs the training sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "## 1.1 Loading Training Data (For Input Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3443</th>\n",
       "      <td>3443</td>\n",
       "      <td>足球</td>\n",
       "      <td>利物浦重賽擊敗乙組仔　英足盃過關 英格蘭足總盃第三圈今晨重賽，貴為英超勁旅的利物浦上場被乙組...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76056</th>\n",
       "      <td>76056</td>\n",
       "      <td>足球</td>\n",
       "      <td>【中超】恒大「暴力戰」絕殺國安　楊智反重力插水惹爭議（有片） 中超首輪賽事重頭戲，廣州恒大主...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93405</th>\n",
       "      <td>93405</td>\n",
       "      <td>足球</td>\n",
       "      <td>【歐霸決賽】阿積士控球率起腳佔優　隊長卡拉臣輸波不服氣 阿積士以歐洲主要決賽最年輕、平均22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26767</th>\n",
       "      <td>26767</td>\n",
       "      <td>足球</td>\n",
       "      <td>【歐國盃】韋莫斯澄清更衣室未內訌　盼以團結力量挫愛爾蘭 今晚3場直播\\r\\r\\nE組｜比利時...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20843</th>\n",
       "      <td>20843</td>\n",
       "      <td>梁振英</td>\n",
       "      <td>王維基參選　點解？ 王維基在宣布有意出選的記者會上，打出ABC，Anyone But CY的...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id tags                                               text\n",
       "id                                                                  \n",
       "3443    3443   足球  利物浦重賽擊敗乙組仔　英足盃過關 英格蘭足總盃第三圈今晨重賽，貴為英超勁旅的利物浦上場被乙組...\n",
       "76056  76056   足球  【中超】恒大「暴力戰」絕殺國安　楊智反重力插水惹爭議（有片） 中超首輪賽事重頭戲，廣州恒大主...\n",
       "93405  93405   足球  【歐霸決賽】阿積士控球率起腳佔優　隊長卡拉臣輸波不服氣 阿積士以歐洲主要決賽最年輕、平均22...\n",
       "26767  26767   足球  【歐國盃】韋莫斯澄清更衣室未內訌　盼以團結力量挫愛爾蘭 今晚3場直播\\r\\r\\nE組｜比利時...\n",
       "20843  20843  梁振英  王維基參選　點解？ 王維基在宣布有意出選的記者會上，打出ABC，Anyone But CY的..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.read_csv('offsite-test-material/offsite-tagging-training-set.csv', encoding='utf8')\n",
    "training_df.index = training_df['id']\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, 'html5lib') #remove HTML tokens\n",
    "    text_only = soup.get_text() \n",
    "    text_normal_newline = re.sub(\"\\n\\n+\", \"\\n\", text_only)\n",
    "    text_normal_space = re.sub(\"\\s\\s+\", \" \", text_normal_newline)\n",
    "    return text_normal_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>clean_length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3443</th>\n",
       "      <td>3443</td>\n",
       "      <td>足球</td>\n",
       "      <td>利物浦重賽擊敗乙組仔　英足盃過關 英格蘭足總盃第三圈今晨重賽，貴為英超勁旅的利物浦上場被乙組...</td>\n",
       "      <td>利物浦重賽擊敗乙組仔　英足盃過關 英格蘭足總盃第三圈今晨重賽，貴為英超勁旅的利物浦上場被乙組...</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76056</th>\n",
       "      <td>76056</td>\n",
       "      <td>足球</td>\n",
       "      <td>【中超】恒大「暴力戰」絕殺國安　楊智反重力插水惹爭議（有片） 中超首輪賽事重頭戲，廣州恒大主...</td>\n",
       "      <td>【中超】恒大「暴力戰」絕殺國安　楊智反重力插水惹爭議（有片） 中超首輪賽事重頭戲，廣州恒大主...</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93405</th>\n",
       "      <td>93405</td>\n",
       "      <td>足球</td>\n",
       "      <td>【歐霸決賽】阿積士控球率起腳佔優　隊長卡拉臣輸波不服氣 阿積士以歐洲主要決賽最年輕、平均22...</td>\n",
       "      <td>【歐霸決賽】阿積士控球率起腳佔優　隊長卡拉臣輸波不服氣 阿積士以歐洲主要決賽最年輕、平均22...</td>\n",
       "      <td>948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26767</th>\n",
       "      <td>26767</td>\n",
       "      <td>足球</td>\n",
       "      <td>【歐國盃】韋莫斯澄清更衣室未內訌　盼以團結力量挫愛爾蘭 今晚3場直播\\r\\r\\nE組｜比利時...</td>\n",
       "      <td>【歐國盃】韋莫斯澄清更衣室未內訌　盼以團結力量挫愛爾蘭 今晚3場直播\\nE組｜比利時Vs愛爾...</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20843</th>\n",
       "      <td>20843</td>\n",
       "      <td>梁振英</td>\n",
       "      <td>王維基參選　點解？ 王維基在宣布有意出選的記者會上，打出ABC，Anyone But CY的...</td>\n",
       "      <td>王維基參選　點解？ 王維基在宣布有意出選的記者會上，打出ABC，Anyone But CY的...</td>\n",
       "      <td>1239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id tags                                               text  \\\n",
       "id                                                                     \n",
       "3443    3443   足球  利物浦重賽擊敗乙組仔　英足盃過關 英格蘭足總盃第三圈今晨重賽，貴為英超勁旅的利物浦上場被乙組...   \n",
       "76056  76056   足球  【中超】恒大「暴力戰」絕殺國安　楊智反重力插水惹爭議（有片） 中超首輪賽事重頭戲，廣州恒大主...   \n",
       "93405  93405   足球  【歐霸決賽】阿積士控球率起腳佔優　隊長卡拉臣輸波不服氣 阿積士以歐洲主要決賽最年輕、平均22...   \n",
       "26767  26767   足球  【歐國盃】韋莫斯澄清更衣室未內訌　盼以團結力量挫愛爾蘭 今晚3場直播\\r\\r\\nE組｜比利時...   \n",
       "20843  20843  梁振英  王維基參選　點解？ 王維基在宣布有意出選的記者會上，打出ABC，Anyone But CY的...   \n",
       "\n",
       "                                              text_clean  clean_length  \n",
       "id                                                                      \n",
       "3443   利物浦重賽擊敗乙組仔　英足盃過關 英格蘭足總盃第三圈今晨重賽，貴為英超勁旅的利物浦上場被乙組...           369  \n",
       "76056  【中超】恒大「暴力戰」絕殺國安　楊智反重力插水惹爭議（有片） 中超首輪賽事重頭戲，廣州恒大主...           631  \n",
       "93405  【歐霸決賽】阿積士控球率起腳佔優　隊長卡拉臣輸波不服氣 阿積士以歐洲主要決賽最年輕、平均22...           948  \n",
       "26767  【歐國盃】韋莫斯澄清更衣室未內訌　盼以團結力量挫愛爾蘭 今晚3場直播\\nE組｜比利時Vs愛爾...           770  \n",
       "20843  王維基參選　點解？ 王維基在宣布有意出選的記者會上，打出ABC，Anyone But CY的...          1239  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df['text_clean'] = training_df.apply(lambda _: remove_html(_['text']), axis=1)\n",
    "training_df['clean_length'] = training_df.apply(lambda _: len(_['text_clean']), axis=1)\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_cnt</th>\n",
       "      <th>record_cnt</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tags</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>梁振英</th>\n",
       "      <td>868598</td>\n",
       "      <td>929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>美國大選</th>\n",
       "      <td>972470</td>\n",
       "      <td>842</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>足球</th>\n",
       "      <td>1672172</td>\n",
       "      <td>2123</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      char_cnt  record_cnt  label_id\n",
       "tags                                \n",
       "梁振英     868598         929         0\n",
       "美國大選    972470         842         1\n",
       "足球     1672172        2123         2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_group(row):\n",
    "    return pd.Series(dict(char_cnt=row['clean_length'].sum(), record_cnt=row.clean_length.count()))\n",
    "labels_df = pd.DataFrame(training_df.groupby(['tags']).apply(process_group))\n",
    "labels_df['label_id'] = pd.Categorical(labels_df.index).codes\n",
    "label_dict = {a: b.label_id for a, b in labels_df.iterrows()}\n",
    "label_id_dict = {b.label_id: a for a, b in labels_df.iterrows()}\n",
    "labels = list(label_dict.keys())\n",
    "labels_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Loading Data (For Frequency Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_dict = defaultdict(list)\n",
    "with open('offsite-test-material/offsite-tagging-training-set.csv', 'r', encoding='utf8') as f:\n",
    "    file_reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    next(file_reader)\n",
    "    for row in file_reader:\n",
    "        text_dict[row[1]].append(remove_html(row[2]))\n",
    "\n",
    "fulltext_dict = {k: '\\n'.join([_ for _ in v]) for k, v in text_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following categories:\n",
      "足球: 2123 fragments with 1674294 characters\n",
      "梁振英: 929 fragments with 869526 characters\n",
      "美國大選: 842 fragments with 973311 characters\n"
     ]
    }
   ],
   "source": [
    "print('Found the following categories:\\n{}'.format('\\n'.join(['{}: {} fragments with {} characters'\n",
    "                                                               .format(k, len(text_dict[k]), len(fulltext_dict[k]))\n",
    "                                                               for k in text_dict.keys() ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we have twice the frequency of articles related to soccer than to either the outgoing CE or US elections. This is a bit tricky in terms of maximum TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Selecting most relevant tokens\n",
    "I am building a TD-IDF-esque model, for which I will select the most 'relevant' tokens as features. Relevance here is defined as the highest ratio of frequency in the relevant 'term' over the frequency in the overall 'document'. A 'term' here is the union of all segments that belong to a single categroy. The document is the union of all segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dictionary for occurrence of short tokens in each classified doc\n",
    "labeldicts_short = {_: (defaultdict(float), 0) for _ in labels} \n",
    "#dictionary for occurrence of long tokens in each classified doc\n",
    "labeldicts_long = {_: (defaultdict(float), 0) for _ in labels} \n",
    "# dictionary for occurrence of short tokens in the whole document\n",
    "docdict_short = defaultdict(float), 0 \n",
    "# dictionary for occurrence of long tokens in the whole document\n",
    "docdict_long = defaultdict(float), 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/9d/qql6_x6575d88_7f44mgptw40000gp/T/jieba.cache\n",
      "Loading model cost 0.820 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# Counting token frequency\n",
    "training_clean = list()\n",
    "for label, combined_text in fulltext_dict.items():\n",
    "    short_tokens = jieba.cut(combined_text, cut_all=True)\n",
    "    for token in short_tokens:\n",
    "        labeldicts_short[label][0][token] += 1\n",
    "        docdict_short[0][token] += 1\n",
    "        \n",
    "    long_tokens = jieba.cut(combined_text, cut_all=False)\n",
    "    for token in long_tokens:\n",
    "        labeldicts_long[label][0][token] += 1\n",
    "        docdict_long[0][token] += 1\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating term/document length\n",
    "for label in labels:\n",
    "    labeldicts_long[label] = labeldicts_long[label][0], sum(labeldicts_long[label][0].values())\n",
    "    labeldicts_short[label] = labeldicts_short[label][0], sum(labeldicts_short[label][0].values())\n",
    "    \n",
    "docdict_long = docdict_long[0], sum(docdict_long[0].values())\n",
    "docdict_short = docdict_short[0], sum(docdict_short[0].values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0907319330176026, 29.0, 29.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper function that returns the highest TFIDF of a token. \n",
    "# highly relevant tokens will have maximum TDIDFs of 2-3,  \n",
    "# they exclusively occur in fragments of one class, but the size of the term length differs\n",
    "# irrelevant tokens will have uniform TFIDFs of 1 (they occur everywhere with the same frequencyy)\n",
    "def relative_frequency(token, classdicts, docdict, docdict_total=None):\n",
    "    occurrences = [(classdict[0][token], classdict[1]) for classdict in classdicts if token in classdict[0]]\n",
    "    if occurrences:\n",
    "        max_occurence, term_length = max(occurrences, key=lambda _: _[0]/_[1])\n",
    "        total_occurrence, doc_length  = docdict[0][token], docdict[1]\n",
    "        tf = (max_occurence/term_length)\n",
    "        df = (total_occurrence/doc_length)\n",
    "        return (tf/df, max_occurence, total_occurrence)\n",
    "    else:\n",
    "        print(token)\n",
    "        return 0, 0, docdict[0].get(token, 0)\n",
    "\n",
    "relative_frequency('重賽', labeldicts_long.values(), docdict_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_classdicts = labeldicts_short.values()\n",
    "long_classdicts = labeldicts_long.values()\n",
    "maxfreq_short = {key: relative_frequency(key, short_classdicts, docdict_short) for key in docdict_short[0].keys()}\n",
    "maxfreq_long = {key: relative_frequency(key, long_classdicts, docdict_long) for key in docdict_long[0].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_tokens_short_list = sorted([key for key, value in maxfreq_short.items() \n",
    "                                     if (value[0] > relevance_cutoff and\n",
    "                                         value[2] > occurrent_cutoff and\n",
    "                                         key.isalpha())])\n",
    "\n",
    "relevant_tokens_long_list = sorted([key for key, value in maxfreq_long.items() \n",
    "                                    if (value[0] > relevance_cutoff and\n",
    "                                        value[2] > occurrent_cutoff and\n",
    "                                        key.isalpha())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Creating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentence, tokenlist, cut_all=False):\n",
    "    a = defaultdict(int)\n",
    "    tokens = jieba.cut(sentence, cut_all=cut_all)\n",
    "    for token in tokens:\n",
    "        a[token] += 1\n",
    "    out_dict = {_: a.get(_, 0) for _ in tokenlist}\n",
    "    return pd.Series(out_dict)\n",
    "\n",
    "occ_input_long = pd.DataFrame(training_df.text_clean.apply(\n",
    "    lambda _: sentence_to_vector(_, relevant_tokens_long_list)))\n",
    "\n",
    "occ_input_short = pd.DataFrame(training_df.text_clean.apply(\n",
    "    lambda _: sentence_to_vector(_, relevant_tokens_short_list, cut_all=True)))\n",
    "\n",
    "data_target = pd.DataFrame(training_df.merge(labels_df, how='inner', left_on='tags', right_index=True)['label_id'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = list(occ_input_long.index)\n",
    "random.seed(random_seed)\n",
    "test_indices = random.sample(indices, int(len(indices)*test_share))\n",
    "train_indices = [_ for _ in indices if _ not in test_indices]\n",
    "training_data_long = occ_input_long.loc[train_indices]\n",
    "training_data_short = occ_input_short.loc[train_indices]\n",
    "holdout_data_long = occ_input_long.loc[test_indices]\n",
    "holdout_data_short = occ_input_short.loc[test_indices]\n",
    "training_target = data_target.loc[train_indices]\n",
    "holdout_target = data_target.loc[test_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Training Models\n",
    "## 2.0 Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_misclassification(id_, prediction, holdout):\n",
    "    relevant_text = training_df.loc[id_]['text_clean']\n",
    "    provided_label = training_df.loc[id_]['tags']\n",
    "    predicted_label = label_id_dict[prediction[holdout.index.get_loc(id_)]]\n",
    "    print('The following text was classified as {0}, but labelled as {1}:\\n{2}'\n",
    "          .format(predicted_label, provided_label, relevant_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Standard RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A RandomForest Classifier reached an accuracy score of 0.9923 for short tokens.\n",
      "This means that a total of 6 fragments (out of 778 fragments in the holdout sample) was misclassified.\n",
      "The misclassified ids are:\n",
      "14227, 14792, 47772, 51805, 58992, 80645\n"
     ]
    }
   ],
   "source": [
    "RFC_short = RandomForestClassifier()\n",
    "RFC_short.fit(np.asarray(training_data_short), np.asarray(training_target).ravel())\n",
    "rfc_prediction_short = RFC_short.predict(np.asarray(holdout_data_short))\n",
    "misclassified_ids_short = list(sorted(holdout_target[rfc_prediction_short!=holdout_target['label_id']].index))\n",
    "print('A RandomForest Classifier reached an accuracy score of {0:.4f} for short tokens.\\nThis means that a total of {1}'\n",
    "      ' fragments (out of {2} fragments in the holdout sample) was misclassified.\\nThe misclassified ids are:\\n{3}'\n",
    "      .format(accuracy_score(rfc_prediction_short, holdout_target),\n",
    "              len(misclassified_ids_short),\n",
    "              len(holdout_target),\n",
    "             ', '.join(str(_) for _ in misclassified_ids_short)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A RandomForest Classifier reached an accuracy score of 0.9910 for short tokens.\n",
      "This means that a total of 7 fragments (out of 778 fragments in the holdout sample) was misclassified.\n",
      "The misclassified ids are:\n",
      "54209, 14792, 47772, 47893, 51805, 1160, 61971\n"
     ]
    }
   ],
   "source": [
    "RFC_long = RandomForestClassifier()\n",
    "RFC_long.fit(np.asarray(training_data_long), np.asarray(training_target).ravel())\n",
    "rfc_prediction_long = RFC_long.predict(np.asarray(holdout_data_long))\n",
    "misclassified_ids_long = list(holdout_target[rfc_prediction_long!=holdout_target['label_id']].index)\n",
    "print('A RandomForest Classifier reached an accuracy score of {0:.4f} for short tokens.\\nThis means that a total of {1}'\n",
    "      ' fragments (out of {2} fragments in the holdout sample) was misclassified.\\nThe misclassified ids are:\\n{3}'\n",
    "      .format(accuracy_score(rfc_prediction_long, holdout_target),\n",
    "              len(misclassified_ids_long),\n",
    "              len(holdout_target),\n",
    "             ', '.join(str(_) for _ in misclassified_ids_long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following text was classified as 梁振英, but labelled as 足球:\n",
      "傑志中心剔出改劃　區議員指原區重置無可能　仍有機會回收場地 沙田區議會發展及房委會將於本周四（11月3日）討論安睦街資助房屋發展計劃。根據規劃署最新文件，傑志中心地皮已被剔出改劃建議，至於中心現時空置的北面用地，政府則建議由「休憩用地」改劃為「住宅」（約0.43公頃），最大住用總樓面面積不超過約2. 沙田區議會發展及房委會將於本周四（11月3日）討論安睦街資助房屋發展計劃。根據規劃署最新文件，傑志中心地皮已被剔出改劃建議，至於中心現時空置的北面用地，政府則建議由「休憩用地」改劃為「住宅」（約0.43公頃），最大住用總樓面面積不超過約2.6萬平方米，建築物高度限為32層內，預料可提供約560個單位。\n",
      "不過「魔鬼細節」卻在註腳上，文件細字上寫明現時傑志中心只作短期租用，政府另覓土地重置訓練中心及在落實遷置後，便展開改劃工作，再次呼應特首梁振英早前指傑志中心重置後始收地的言論。\n",
      "延伸閱讀：政府擬收回石門傑志中心建資助出售房屋 明年約滿可合法收地\n",
      "在facebook披露政府摸底工作的沙田區議員容溟舟接受《香港01》訪問時表示，傑志要在沙田區重置「近乎無可能」，因附近平地都已正在興建住宅，今次政府剔出改劃範圍只屬短期措施，長遠亦有機會等待該地於2017年9月租約期滿後再回收亦合法。\n"
     ]
    }
   ],
   "source": [
    "misclassified_id = misclassified_ids_short[0]\n",
    "explain_misclassification(misclassified_id, rfc_prediction_short, holdout_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Gradient Boosted Classifier (Standard SkLearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A GradientBoosted tree ensemble Classifier reached an accuracy score of 0.9884 for short tokens.\n",
      "This means that a total of 778 fragments (out of 778 fragments in the holdout sample) was misclassified.\n",
      "The misclassified ids are:\n",
      "5375, 14792, 23475, 47772, 51805, 53293, 58992, 80645, 88049\n"
     ]
    }
   ],
   "source": [
    "GBC_short = GradientBoostingClassifier()\n",
    "GBC_short.fit(np.asarray(training_data_short), np.asarray(training_target).ravel())\n",
    "gbc_prediction_short = GBC_short.predict(np.asarray(holdout_data_short))\n",
    "misclassified_ids_short = list(sorted(holdout_target[gbc_prediction_short!=holdout_target['label_id']].index))\n",
    "print('A GradientBoosted tree ensemble Classifier reached an accuracy score of {0:.4f} for short tokens.'\n",
    "      '\\nThis means that a total of {1}'\n",
    "      ' fragments (out of {2} fragments in the holdout sample) was misclassified.\\nThe misclassified ids are:\\n{3}'\n",
    "      .format(accuracy_score(gbc_prediction_short, holdout_target),\n",
    "              len(gbc_prediction_short),\n",
    "              len(holdout_target),\n",
    "             ', '.join(str(_) for _ in misclassified_ids_short)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "misclassified_id = misclassified_ids_short[0]\n",
    "#explain_misclassification(misclassified_id, gbc_prediction_short, holdout_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A GradientBoosted tree ensemble Classifier reached an accuracy score of 0.9897 for short tokens.\n",
      "This means that a total of 8 fragments (out of 778 fragments in the holdout sample) was misclassified.\n",
      "The misclassified ids are:\n",
      "14792, 45215, 47772, 47893, 51805, 51851, 54209, 80645\n"
     ]
    }
   ],
   "source": [
    "GBC_long = GradientBoostingClassifier()\n",
    "GBC_long.fit(np.asarray(training_data_long), np.asarray(training_target).ravel())\n",
    "gbc_prediction_long = GBC_long.predict(np.asarray(holdout_data_long))\n",
    "misclassified_ids_long = list(sorted(holdout_target[gbc_prediction_long!=holdout_target['label_id']].index))\n",
    "print('A GradientBoosted tree ensemble Classifier reached an accuracy score of {0:.4f} for short tokens.'\n",
    "      '\\nThis means that a total of {1}'\n",
    "      ' fragments (out of {2} fragments in the holdout sample) was misclassified.\\nThe misclassified ids are:\\n{3}'\n",
    "      .format(accuracy_score(gbc_prediction_long, holdout_target),\n",
    "              len(misclassified_ids_long),\n",
    "              len(holdout_target),\n",
    "             ', '.join(str(_) for _ in misclassified_ids_long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following text was classified as 足球, but labelled as 梁振英:\n",
      "【向奧巴馬學習】要人支持你　首先你要真心支持本地體育 要得到人真心支持，不是單單只看 8 分鐘的比賽便當自己支持本地體育……不知道新上任的體育專員楊德強先生，會否願意抽時間和市民一齊排隊等買飛，了解香港籃壇生態，做一個不離地的高官？邱益忠 拜讀馬嶽教授一篇「當抽足球水抽火水 拜讀馬嶽教授一篇「當抽足球水抽火水」後，筆者第一個想起的人是美國總統奧巴馬。成也危機公關，奧巴馬極擅長在莊嚴與幽默之間取得平衡，其化危為機的能力甚高，如果他退任後決定開班傳授「化解關公災難」，相信不少人會爭相報讀。\n",
      "忠實球迷身分　營造親民形象\n",
      "真．熱愛體育的奧巴馬，對足球、棒球、高爾夫、網球、乒乓球、保齡球等運動相當熟悉，他更是芝加哥公牛的瘋狂球迷，每年被問到哪支球隊是總冠軍熱門，總是堅定不移地回答：「公牛」。\n",
      "自「籃球之神」Michael Jordan 退休後，公牛隊一直陷入漫長的重建期，若非近幾年 Derrick Rose 無法躲過輪迴的傷病，公牛隊早就重回顛峰，想到這點，公牛球迷總掩不住失望。\n",
      "「原來總統也會有評估錯誤的時候？」\n",
      "「原來總統和我一樣，不離不棄支持家鄉球隊？」\n",
      "鐵血球迷的本色，成功把奧巴馬和普通市民的距離拉近不少。 抽水功力深厚　搞氣氛能手\n",
      "每年贏得美國四大聯賽的冠軍球隊（NBA 籃球、NFL 美式足球、NHL 冰上曲棍球、MLB 棒球），都會獲得美國總統在白宮接見，這絕對是運動員的最高榮譽。連在 NBA 要風得風、一向自信爆棚、每天在床上「被帥醒」的大帝 Lebron James，在白宮發言時也不禁「口窒窒」，甚至露出童真一面，高呼「Mama，I did it!」。\n",
      "每次在白宮接見 NBA 冠軍球隊，奧巴馬不會阿諛奉承，反而不斷「搵位入」，大讚自己最愛的公牛隊。如數年前湖人到訪白宮時，奧巴馬非常「識做」，先祝賀當時教練 Phil Jackson 贏得第 10 次總冠軍，成就史上第一的戰績；但立刻鬼馬地補充一句：「不過其中 6 個冠軍是在公牛拿下的，記得嗎，Magic Johnson？」讓 Magic Johnson 哭笑不得。\n",
      "2014 年兩連冠的熱火創下史上第 2 多的 27 連勝，奧巴馬不忘抽水：「27 連勝的紀錄非常了不起，幾乎可和公牛的 72 勝相比。」如此明正言順地力挺自己愛隊的「讚賞」方式，至少比一些見高拜的勢利球迷更值得尊敬。\n",
      "以為奧巴馬只留意超級球星？你錯了。沒想到奧巴馬（或背後的幕僚）相當留意球場以外的花邊新聞。\n",
      "話說 Mario Chalmers 在熱火時期是 Lebron James 的出氣袋，每次戰術執行不力時，總會被 Lebron James 罵得狗血淋頭，而奧巴馬和熱火隊員合照前幽大帝一默，笑說：「拍大合照要快一點，否則某位人兄又會走向 Chalmers 怒吼了。」聽到總統點名諷刺，熱火隊員立刻爆笑，中槍的大帝只能尷尬得把頭挨在 Wade 的肩膀上。\n",
      "小牛一哥 Dirk Nowitzki 聲音低沈，經常唱歌走音，也少不免被奧巴馬抽水：「對 Dirk 最痛苦的事，莫過於要他在冠軍遊行時唱『We Are the Champions』……」 向奧巴馬學習　不做離地高官\n",
      "從上述例子不難明白，為何奧巴馬在 2008 年美國大選期間，得到超過七成 NBA 球員的支持。當時巫師一哥 Gilbert Arenas 將奧巴馬的競選口號「Change We Believe In」紋在左手手指上；Lebron James 及美國 Hip Hop 天皇 Jay-Z 更身體力行，舉辦音樂會力撐奧巴馬。\n",
      "要得到人真心支持，不是單單只看 8 分鐘的比賽便當自己支持本地體育。\n",
      "香港籃壇掀起一波波熱潮，業餘籃球聯賽開得成行成市，而最近香港甲一籃球聯賽的比賽更是一票難求！\n",
      "上月永倫對決東方的銀牌初賽，2,000 張門票極速售罄，網上直播人數更超過 8 萬人次，這股熱潮實屬難得，不知道新上任的體育專員楊德強先生，會否願意抽時間和市民一齊排隊等買飛，了解香港籃壇生態，做一個不離地的高官？\n"
     ]
    }
   ],
   "source": [
    "misclassified_id = misclassified_ids_long[0]\n",
    "#explain_misclassification(misclassified_id, gbc_prediction_long, holdout_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Support Vector Machine with Cosine Similarity Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A SupportVectorMachine with CosineSimilarity kernel reached an accuracy score of 0.9961 for short tokens.\n",
      "This means that a total of 778 fragments (out of 778 fragments in the holdout sample) was misclassified.\n",
      "The misclassified ids are:\n",
      "14792, 47772, 51805\n"
     ]
    }
   ],
   "source": [
    "svc_short = SVC(kernel=cosine_similarity)\n",
    "svc_short.fit(np.asarray(training_data_short), np.asarray(training_target).ravel())\n",
    "svc_prediction_short = svc_short.predict(np.asarray(holdout_data_short))\n",
    "misclassified_ids_short = list(sorted(holdout_target[svc_prediction_short!=holdout_target['label_id']].index))\n",
    "print('A SupportVectorMachine with CosineSimilarity kernel reached an accuracy score of {0:.4f} for short tokens.'\n",
    "      '\\nThis means that a total of {1}'\n",
    "      ' fragments (out of {2} fragments in the holdout sample) was misclassified.\\nThe misclassified ids are:\\n{3}'\n",
    "      .format(accuracy_score(svc_prediction_short, holdout_target),\n",
    "              len(svc_prediction_short),\n",
    "              len(holdout_target),\n",
    "             ', '.join(str(_) for _ in misclassified_ids_short)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following text was classified as 足球, but labelled as 梁振英:\n",
      "【向奧巴馬學習】要人支持你　首先你要真心支持本地體育 要得到人真心支持，不是單單只看 8 分鐘的比賽便當自己支持本地體育……不知道新上任的體育專員楊德強先生，會否願意抽時間和市民一齊排隊等買飛，了解香港籃壇生態，做一個不離地的高官？邱益忠 拜讀馬嶽教授一篇「當抽足球水抽火水 拜讀馬嶽教授一篇「當抽足球水抽火水」後，筆者第一個想起的人是美國總統奧巴馬。成也危機公關，奧巴馬極擅長在莊嚴與幽默之間取得平衡，其化危為機的能力甚高，如果他退任後決定開班傳授「化解關公災難」，相信不少人會爭相報讀。\n",
      "忠實球迷身分　營造親民形象\n",
      "真．熱愛體育的奧巴馬，對足球、棒球、高爾夫、網球、乒乓球、保齡球等運動相當熟悉，他更是芝加哥公牛的瘋狂球迷，每年被問到哪支球隊是總冠軍熱門，總是堅定不移地回答：「公牛」。\n",
      "自「籃球之神」Michael Jordan 退休後，公牛隊一直陷入漫長的重建期，若非近幾年 Derrick Rose 無法躲過輪迴的傷病，公牛隊早就重回顛峰，想到這點，公牛球迷總掩不住失望。\n",
      "「原來總統也會有評估錯誤的時候？」\n",
      "「原來總統和我一樣，不離不棄支持家鄉球隊？」\n",
      "鐵血球迷的本色，成功把奧巴馬和普通市民的距離拉近不少。 抽水功力深厚　搞氣氛能手\n",
      "每年贏得美國四大聯賽的冠軍球隊（NBA 籃球、NFL 美式足球、NHL 冰上曲棍球、MLB 棒球），都會獲得美國總統在白宮接見，這絕對是運動員的最高榮譽。連在 NBA 要風得風、一向自信爆棚、每天在床上「被帥醒」的大帝 Lebron James，在白宮發言時也不禁「口窒窒」，甚至露出童真一面，高呼「Mama，I did it!」。\n",
      "每次在白宮接見 NBA 冠軍球隊，奧巴馬不會阿諛奉承，反而不斷「搵位入」，大讚自己最愛的公牛隊。如數年前湖人到訪白宮時，奧巴馬非常「識做」，先祝賀當時教練 Phil Jackson 贏得第 10 次總冠軍，成就史上第一的戰績；但立刻鬼馬地補充一句：「不過其中 6 個冠軍是在公牛拿下的，記得嗎，Magic Johnson？」讓 Magic Johnson 哭笑不得。\n",
      "2014 年兩連冠的熱火創下史上第 2 多的 27 連勝，奧巴馬不忘抽水：「27 連勝的紀錄非常了不起，幾乎可和公牛的 72 勝相比。」如此明正言順地力挺自己愛隊的「讚賞」方式，至少比一些見高拜的勢利球迷更值得尊敬。\n",
      "以為奧巴馬只留意超級球星？你錯了。沒想到奧巴馬（或背後的幕僚）相當留意球場以外的花邊新聞。\n",
      "話說 Mario Chalmers 在熱火時期是 Lebron James 的出氣袋，每次戰術執行不力時，總會被 Lebron James 罵得狗血淋頭，而奧巴馬和熱火隊員合照前幽大帝一默，笑說：「拍大合照要快一點，否則某位人兄又會走向 Chalmers 怒吼了。」聽到總統點名諷刺，熱火隊員立刻爆笑，中槍的大帝只能尷尬得把頭挨在 Wade 的肩膀上。\n",
      "小牛一哥 Dirk Nowitzki 聲音低沈，經常唱歌走音，也少不免被奧巴馬抽水：「對 Dirk 最痛苦的事，莫過於要他在冠軍遊行時唱『We Are the Champions』……」 向奧巴馬學習　不做離地高官\n",
      "從上述例子不難明白，為何奧巴馬在 2008 年美國大選期間，得到超過七成 NBA 球員的支持。當時巫師一哥 Gilbert Arenas 將奧巴馬的競選口號「Change We Believe In」紋在左手手指上；Lebron James 及美國 Hip Hop 天皇 Jay-Z 更身體力行，舉辦音樂會力撐奧巴馬。\n",
      "要得到人真心支持，不是單單只看 8 分鐘的比賽便當自己支持本地體育。\n",
      "香港籃壇掀起一波波熱潮，業餘籃球聯賽開得成行成市，而最近香港甲一籃球聯賽的比賽更是一票難求！\n",
      "上月永倫對決東方的銀牌初賽，2,000 張門票極速售罄，網上直播人數更超過 8 萬人次，這股熱潮實屬難得，不知道新上任的體育專員楊德強先生，會否願意抽時間和市民一齊排隊等買飛，了解香港籃壇生態，做一個不離地的高官？\n"
     ]
    }
   ],
   "source": [
    "misclassified_id = misclassified_ids_short[0]\n",
    "explain_misclassification(misclassified_id, svc_prediction_short, holdout_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
