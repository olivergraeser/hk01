{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import jieba.analyse\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('offsite-test-material/offsite-tagging-training-set.csv', 'r', encoding='utf8') as f:\n",
    "    file_reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    next(file_reader)\n",
    "    file_columns = [(int(_[0]),_[1], _[2]) for _ in file_reader] #id, class, text\n",
    "\n",
    "with open('offsite-test-material/offsite-tagging-test-set.csv', 'r', encoding='utf8') as f:\n",
    "    file_reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    next(file_reader)\n",
    "    test_columns = [(int(_[0]),_[1]) for _ in file_reader] #id, class, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = {_[1] for _ in file_columns}\n",
    "labeldicts_small = {_: defaultdict(float) for _ in labels}\n",
    "labeldicts_large = {_: defaultdict(float) for _ in labels}\n",
    "docdict = defaultdict(float)\n",
    "longdocdict = defaultdict(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.942 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "def max_if_exists(token, classdicts, docdict):\n",
    "    max_occurence = max([classdict[token] for classdict in classdicts if token in classdict])\n",
    "    total_occurrence = docdict[token]\n",
    "    return max_occurence/total_occurrence, max_occurence, total_occurrence\n",
    "\n",
    "\n",
    "for training in file_columns:\n",
    "    soup = BeautifulSoup(training[2], 'html5lib')\n",
    "    text_only = soup.get_text()\n",
    "    tokens = jieba.cut(text_only, cut_all=True)\n",
    "    long_tokens = jieba.cut(text_only, cut_all=False)\n",
    "    for token in tokens:\n",
    "        labeldicts_small[training[1]][token] += 1\n",
    "        docdict[token] += 1\n",
    "    for token in long_tokens:\n",
    "        labeldicts_large[training[1]][token] += 1\n",
    "        longdocdict[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_occurence = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_classdicts = labeldicts_small.values()\n",
    "long_classdicts = labeldicts_large.values()\n",
    "maxfreq_short = {key: max_if_exists(key, short_classdicts, docdict) for key in docdict.keys()}\n",
    "maxfreq_long = {key: max_if_exists(key, long_classdicts, longdocdict) for key in longdocdict.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token2vector_set = {_ for _ in maxfreq_long.items() if _[1][0]>=.74 and _[1][1] > min_occurence}\n",
    "token2vector_dict = {_[0]:i for i, _ in enumerate(token2vector_set)}\n",
    "vector_dimesion = len(token2vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def tokens_to_vector(tokens):\n",
    "    vector = np.zeros(vector_dimesion)\n",
    "    for token in tokens:\n",
    "        if token in token2vector_dict:\n",
    "            vector[token2vector_dict[token]] += 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentence, cut_all=False, needs_soup=False):\n",
    "    tokens = jieba.cut(sentence, cut_all=cut_all)\n",
    "    return tokens_to_vector(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"菲力斯是現南華門將摩拉的前國家隊隊友，兩人曾一同征戰2006年德國世界盃，他該屆只在揭幕戰對德國時上陣。菲力斯曾效力國內外多支球隊，最輝煌的時期應為2007-10年效力德乙緬恩斯時，他上陣55場射入23球；去年則曾投效美職的美國芝華士，上陣12次只入3球；今年則返回國內球隊落班。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence_to_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fa140bf26dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence_to_vector' is not defined"
     ]
    }
   ],
   "source": [
    "sentence_to_vector(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "highest_freq_list = []\n",
    "for training in file_columns:\n",
    "    soup = BeautifulSoup(training[2], 'html5lib')\n",
    "    text_only = soup.get_text()\n",
    "    tokens = jieba.cut(text_only, cut_all=True)\n",
    "    long_tokens = jieba.cut(text_only, cut_all=True)\n",
    "    highest_freq_short = max([(_, maxfreq_short[_][0], maxfreq_short[_][2]) for _ in tokens if maxfreq_short[_][2] >min_occurence], key=lambda _: _[1])\n",
    "    highest_freq_long = max([(_, maxfreq_long[_][0], maxfreq_long[_][2]) for _ in long_tokens if maxfreq_long[_][2] >min_occurence], key=lambda _: _[1])\n",
    "    highest_freq_list.append((training[0], highest_freq_long, highest_freq_short))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sorted(highest_freq_list, key=lambda _: _[2][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len({_[1][0] for _ in highest_freq_list} | {_[2][0] for _ in highest_freq_list}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{_[1][0] for _ in highest_freq_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3098b42909a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcv_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'min_child_weight'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m ind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n\u001b[1;32m      4\u001b[0m              'objective': 'binary:logistic'}\n\u001b[1;32m      5\u001b[0m optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\n",
    "ind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "             'objective': 'binary:logistic'}\n",
    "optimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n",
    "                            cv_params, \n",
    "                             scoring = 'accuracy', cv = 5, n_jobs = -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  0.  2.  0.] 3.0 5.0 [ 0.  1.  0.  3.  0.] 4.0 10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  6.,  0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.zeros(5)\n",
    "a[1]=1\n",
    "a[3]=2\n",
    "b=np.zeros(5)\n",
    "b[1]=1\n",
    "b[3]=3\n",
    "print(a,sum(a),sum(a*a),b,sum(b),sum(b*b))\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
